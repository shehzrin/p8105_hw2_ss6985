P8105 Homework 2
================
Shehzrin Shah
2024-09-28

## Problem 1

Import data, clean data, handle missing data, select data, mutate data:

``` r
subway_df = 
  read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA",".","")) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

### Description of Dataset:

The NYC Transit Subway dataset contains information about subway station
entrances and exits in New York City. The data has been **imported** and
the column names have been **cleaned** to be in a common standard format
(e.g., through underscores for spaces/lowercase consistency). I
**handled missing values** by converting “NA”, “.”, and empty strings to
NA. I **selected** specific columns/variables that are relevant for
further analysis, including the variables `line` (subway line),
`station_name` (name of the station), `station_latitude` and
`station_longitude` (geographic coordinates of the station), routes
served (up to 11 routes per station), `entrance_type` (type of entrance,
e.g., stair, elevator, etc.), `entry` (whether the entrance allows
entry), `vending` (whether there is vending), and `ada` (whether the
station is ADA compliant). The `entry` variable was converted from a
character (“YES”/“NO”) to a logical variable (TRUE for “YES”, FALSE for
“NO”). The resulting dataset has **1868 rows** and **19 columns**. The
data is **not tidy**, considering there are several columns for each
route number, for a total of 11 routes/columns for routes served.

### Questions About Data:

``` r
distinct_stations = 
  subway_df |>
  distinct(line, station_name) |>
  nrow()
```

There are **465** distinct stations.

``` r
ada_compliant_stations = subway_df |>
  filter(ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow() |>
  print()
```

    ## [1] 84

There are **84** stations that are ADA compliant.

``` r
no_vending_entry = subway_df |>
  filter(vending == "NO", entry == TRUE) |>
  nrow() |>
  print()
```

    ## [1] 69

There are 69 station entrances/exits without vending that allow
entrance.

``` r
no_vending = subway_df |>
  filter(vending == "NO")|>
  nrow() |>
  print()
```

    ## [1] 183

There are 1868 station entrances/exits without vending. **69/183 or
37.70%** of station entrances/exits without vending allow entrance.

### Reformatting Data:

``` r
subway_long_df = 
  subway_df |>
  mutate(
    across(
      starts_with("route"), as.character)
    )|> 
  pivot_longer(
    cols = starts_with("route"), 
    names_to = "route_number", 
    values_to = "route_name",
    values_drop_na = TRUE
    ) 

distinct_a_train_stations = subway_long_df |>
  filter(route_name == "A") |> 
  distinct(line, station_name) |> 
  nrow() |>
  print()
```

    ## [1] 60

**60** distinct stations serve the A train.

``` r
ada_compliant_a_train_stations = subway_long_df |>
  filter(route_name == "A", ada == TRUE) |> 
  distinct(line, station_name) |> 
  nrow() |>
  print()
```

    ## [1] 17

**17** of the stations that serve the A train are ADA compliant.

## Problem 2

Read and clean the Mr. Trash Wheel sheet:

``` r
mr_trashwheel_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
             na = c("NA",".",""), 
             sheet = "Mr. Trash Wheel", 
             range = "A2:N653"
             ) |>
  janitor::clean_names() |>
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    year = as.integer(year),
    trash_wheel = "mr. trash wheel"
    ) |>
  relocate(trash_wheel, dumpster)
```

Read and clean the Professor Trash Wheel sheet & the Gwynnda Trash Wheel
sheet:

``` r
professor_trashwheel_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
             na = c("NA",".",""), 
             sheet = "Professor Trash Wheel", 
             range = "A2:M120", 
             ) |>
  janitor::clean_names() |>
  mutate(
    trash_wheel = "professor trash wheel",
    year = as.integer(year)
    )

gwynnda_trashwheel_df = 
  read_excel("202409 Trash Wheel Collection Data.xlsx", 
             na = c("NA",".",""), 
             sheet = "Gwynnda Trash Wheel", 
             range = "A2:L265", 
             ) |>
  janitor::clean_names() |>
  mutate(
    trash_wheel = "gwynnda trash wheel",
    year = as.integer(year)
    )
```

Combining datasets:

``` r
trashwheel_df = 
  bind_rows(mr_trashwheel_df, professor_trashwheel_df, gwynnda_trashwheel_df) |>
  janitor::clean_names() |>
  relocate(trash_wheel, dumpster) |>
  select(trash_wheel:homes_powered)
```

### About Combined Data:

I imported 3 datasets that each provided information on a single vessel
(Mr.Trash Wheel, Professor Trash Wheel, or Gwynnda Trash Wheel) that
removes trash. Each observation still represents a specific dumpster
collection event. The **variables** included for each observation were
`dumpster` number, `month`, `year`, `date`, `weight` (weight of trash
collected in tons), `volume` (volume of trash collected in cubic yards),
`plastic bottles` (remainder of variables note the number of the
respective object found in the trash collected by the vessel during the
single collection event), `polystyrene`, `cigarette buds`,
`glass bottles` (not included in Gwynnda Trash Wheel dataset),
`plastic bags`, `wrappers`, `sports balls` (not included in Professor
Trash Wheel dataset or Gwynnda Trash Wheel dataset) and `homes powered`.
After cleaning, organizing, and combining the data, the combined dataset
(`trashwheel_df`) contains information from the three vessels (Mr. Trash
Wheel, Professor Trash Wheel, and Gwynnda) with a total of **1032
observations**. The resulting dataset has **1032 rows** and **15
columns**. Each observation still represents a specific dumpster
collection event.

Professor Trash Wheel collected a total of **246.74 tons of trash**. In
June 2022, Gwynnda removed **1.812^{4} cigarette butts** from the water.

## Problem 3

Import bakers dataset:

``` r
bakers_df = 
  read_csv("gbb_datasets/bakers.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |>
  na.omit(bakers_df) |>
  separate(
    baker_name, 
    into = c("baker", "baker_last_name"), 
    sep = " "
  ) |> 
  arrange(series)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Import bakes dataset:

``` r
bakes_df = 
  read_csv("gbb_datasets/bakes.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |>
  mutate(
    baker = str_replace_all(baker, '"Jo"', "Jo")
  )
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Import results dataset:

``` r
results_df = 
  read_csv("gbb_datasets/results.csv", 
           na = c("NA", ".", ""), 
           skip = 2) |>
  janitor::clean_names() |>
  arrange(series,baker) |>
  mutate(
   baker = ifelse(row_number() == 57:64, "Jo", baker)
  )
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

View individual datasets using `anti_join`:

``` r
bakes_missing_df = anti_join(bakers_df, bakes_df)
results_missing_df = anti_join(bakers_df, results_df)
```

Combine the three datasets to create a single dataset:

``` r
gbb_df =
  left_join(bakes_df, results_df, by = c("baker", "series", "episode")) 

gbb_combined_df =
  left_join(gbb_df, bakers_df, by = c("baker", "series")) |>
  relocate(series, episode, baker, baker_last_name)

write_csv(gbb_combined_df, "gbb_datasets/gbb_combined_df.csv")
```

### Data Cleaning Process/Questions:

I loaded the data for the three datasets: Bakers Dataset, Bakes Dataset,
and Results Dataset.

Starting with the Bakers Dataset, I read in the `bakers.csv` dataset
using `read_csv()`, specifying the values to treat as `NA` (`"NA"`,
`"."`, and `""`). The column names were cleaned with
`janitor::clean_names()` for consistent/lowercase naming convention.
Missing values were removed using `na.omit(bakers_df)`. The `baker_name`
column was split into two columns, `baker` (first name) and
`baker_last_name` (last name), using the separate() function. The data
was sorted by the `series` column for proper ordering.

The `bakes.csv` dataset was imported similarly to the bakers dataset,
with the same approach to handling `NA` values. I used the `mutate()`
function to replace any occurrences of `"Jo"` within quotes with `Jo`
without quotes (to standardize name format).

The `results.csv` dataset was read in, but with `skip = 2` to account
for header rows, and the same NA handling was applied. The dataset was
ordered by both `series` and `baker` for consistency. Joanne’s name was
replaced with `"Jo"` using `ifelse()` for consistency.

The `anti_join()` function was used to find missing data.
`bakes_missing_df` was created by performing an anti-join between
`bakers_df` and `bakes_df`, identifying bakers missing from the bakes
dataset.`results_missing_df` was created similarly.

I merged `bakes_df` and `results_df` on common columns `baker`,
`series`, and `episode` using `left_join()`. The combined dataset
(`gbb_df`) was joined with `bakers_df` on `baker` and `series`. The
resulting dataset was reorganized using `relocate()` so that important
variables (`series`, `episode`, `baker`, and `baker_last_name`) appeared
at the beginning. The final combined dataset was saved as a CSV file
using `write_csv()`. I used `na.omit()` to remove missing values. The
final dataset, `gbb_combined_df`, contains all the information from the
three original datasets (`bakes.csv`, `bakers.csv`, and `results.csv`),
including details about bakers, their bakes, and competition results.

The final dataset combines information about bakers, their bakes, and
competition results from the Great British Bake-Off. It includes key
variables such as series, episode, baker names, and details about their
signature, showstopper, and technical challenges, along with the bakers’
personal details (age, occupation, hometown).The resulting dataset has
**548 rows** and **11 columns**. Some values (technical ranking or
showstopper descriptions) have missing data, indicated as “N/A.”

I noticed the joined dataset only includes data from up to series 8
(since the `bakes` dataset only contains information from series 1
through 8).

### Creating a Table:

Create a table showing the star baker or winner of each episode in
Seasons 5 through 10:

``` r
winner_df =
  results_df |> 
  filter(series <= 10, series >=5) |> 
  filter(result %in% c("WINNER", "STAR BAKER"))  |> 
  select(series, episode, baker, result)

winner_df |>
  pivot_wider(
    names_from = series,
    values_from = baker
  ) |> 
  arrange(episode) |> 
  knitr::kable()
```

| episode | result     | 5       | 6      | 7         | 8      | 9       | 10       |
|--------:|:-----------|:--------|:-------|:----------|:-------|:--------|:---------|
|       1 | STAR BAKER | Nancy   | Marie  | Jane      | Steven | Manon   | Michelle |
|       2 | STAR BAKER | Richard | Ian    | Candice   | Steven | Rahul   | Alice    |
|       3 | STAR BAKER | Luis    | Ian    | Tom       | Julia  | Rahul   | Michael  |
|       4 | STAR BAKER | Richard | Ian    | Benjamina | Kate   | Dan     | Steph    |
|       5 | STAR BAKER | Kate    | Nadiya | Candice   | Sophie | Kim-Joy | Steph    |
|       6 | STAR BAKER | Chetna  | Mat    | Tom       | Liam   | Briony  | Steph    |
|       7 | STAR BAKER | Richard | Tamal  | Andrew    | Steven | Kim-Joy | Henry    |
|       8 | STAR BAKER | Richard | Nadiya | Candice   | Stacey | Ruby    | Steph    |
|       9 | STAR BAKER | Richard | Nadiya | Andrew    | Sophie | Ruby    | Alice    |
|      10 | WINNER     | Nancy   | Nadiya | Candice   | Sophie | Rahul   | David    |

The table showing the star bakers and winners during season 5 to season
10 suggests that there was some predictability in overall winners. For
example, Candace, the winner of series 7, was the star baker the most
number of times in comparison to the other bakers in the same series.
Series 10 was the only series that consisted of a winner (David) that
was not also a star baker at least once in their series.

### Viewership Data:

Import, clean, tidy and organize the viewership data:

``` r
viewers_df = 
  read_csv("gbb_datasets/viewers.csv",
           na = c("NA", ".", "")
           ) |>
  janitor::clean_names() |>
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership",
    names_prefix = "series_"
  ) |>
  mutate(
    series = as.numeric(series)
  ) |>
  arrange(series) |>
  relocate (series)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
knitr::kable(head(viewers_df, 10), caption = "First 10 rows of GBB Viewers Dataset")
```

| series | episode | viewership |
|-------:|--------:|-----------:|
|      1 |       1 |       2.24 |
|      1 |       2 |       3.00 |
|      1 |       3 |       3.00 |
|      1 |       4 |       2.60 |
|      1 |       5 |       3.03 |
|      1 |       6 |       2.75 |
|      1 |       7 |         NA |
|      1 |       8 |         NA |
|      1 |       9 |         NA |
|      1 |      10 |         NA |

First 10 rows of GBB Viewers Dataset

``` r
viewers1 = 
  viewers_df |> 
  filter(series == "1") |>
  summarise(mean(viewership, na.rm = TRUE))

viewers5 = 
  viewers_df |> 
  filter(series == "5") |>
  summarise(mean(viewership, na.rm = TRUE))
```

The average viewership in Season 1 was **2.77**. The average viewership
in Season 5 was **10.0393**.
